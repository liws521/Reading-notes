# Meta
- 主要意义在于系统的建立数据压缩的框架, 网上的大多数文章一提到数据压缩就大多将话题扯到"熵, Huffman编码, AC"等主题上, 虽然Huffman很经典, 但明确其在数据压缩的框架中处于哪个位置也很重要, 它只是一棵枝繁叶茂的大树上的一片色彩艳丽的树叶, 还有很多其他方法等待探索
- 扫盲, 在读这本书前我觉得我了解FSE, 读过后觉得我对ANS和FSE一无所知, 了解自己的不了解之处真的是极大的收获.
## C1 并非无趣的一章
- 为了帮助你以尽可能轻松平缓（又有趣）的方式进入数据压缩这一领域，出版者希望我们先介绍一下数据压缩的历史、一些基础知识，还有其他任何我们能想到的内容——但不涉及数学，因为数学很难。但是说实话，如果真那样做的话，不仅读者读起来无趣，作者写的时候也没意思。
    - 这种描述方式很有趣, 对比
- 数据压缩算法有5类：变长编码(variable-length codes，VLC）、统计压缩（statistical compression）、字典编码（dictionary encodings）、上下文模型（context modeling）和多上下文模型（multicontext modeling）
- 对数据进行压缩，通常有两个思路
    - 减少数据中不同符号的数量（即让“字母表”尽可能小）
    - 用更少的位数对更常见的符号进行编码（即最常见的“字母”所用的位数最少）
## C2 不容错过的一章
- 二进制科普
- 根据信息论的观点，一个数值所包含的信息内容等于，为了在一个集合中唯一地确定这个数值，需要做出的二选一（是/否）决定的次数
    - 20 Questions
- 香农将一个变量对应的LOG2函数的值定义为它的熵（entropy），也就是用二进制来表示这个数所需的最少二进制位数。
## C3 突破熵
- 总的来说，一个符号出现得越频繁，它对整个数据集包含的信息内容的贡献就会越少，这看起来似乎完全违背直觉
- 事实上，整个数据压缩科学界的人士都认为熵是互联网上的一大“谎言”
    - 突破熵
    - 真相是，实际上，通过利用真实数据的两个性质，我们完全有可能将数据压缩得比熵定义的还要小。按照香农对熵的定义，他只考虑了符号出现的概率，完全没有考虑符号之间的排序。而对真实数据集来说，排序是一项基本的信息，符号之间的关系同样如此。
- 增量编码（delta coding）
    - 将一系列的数转换为其与上一个数的差后再编码
## C4 VLC(variable-length codess
- Morse Code
    - 手法fist, Morse Code有长脉冲, 短脉冲, 空白, 我们现在只有0/1没有空白
- 一般来说，对数据进行VLC通常有3个步骤。
    - (1) 遍历数据集中的所有符号并计算每个符号的出现概率。
    - (2) 根据概率为每个符号分配码字，一个符号出现的概率越大，所分配的码字就越短。
    - (3) 再次遍历数据集，对每一个符号进行编码，并将对应的码字输出到压缩后的数据流中。
- 为了确保正确，在设计VLC集的码字时，必须考虑两个原则：一是越频繁出现的符号，其对应的码字越短；二是码字需满足前缀性质。
    - 前缀性质, 在某个码字被分配给一个符号之后，其他的码字就不能用该码字作为前缀
## C5 统计编码(statistical encoders)
- 统计编码算法通过数据集中符号出现的概率来进行编码使结果尽可能与熵接近, 也被很多人叫做熵编码
- Huffman
    - 由于创建哈夫曼树（需使用计算资源）要比传输符号码字对应表（会增加数据流大小）困难得多，因此总是应该将码字对应表加在数据流的前面，而不是在解码时再重新创建一次
    - Huffman的问题在于取整带来的损失
    - 按照1∶1的比例为每个符号分配一个整数二进制位长码字
- 算术编码
    - 算术编码的工作原理是将字符串转换为一个数，与字符串相比，表示这个数需要的二进制位数要少一些
    - 划分区间
    - 效果能达到接近熵值, 但浮点运算多, 速度上差
- 非对称数字系统（asymmetric numeral systems，ANS）
    - 实际上，ANS是一种新的精确熵编码方法，所得到的结果可以和最优熵任意接近，它的压缩率与算术编码接近，而性能则与哈夫曼编码相当
    - 这里值得反复观看, 对tANS的更好的理解
- 目前看来未来的路似乎很清楚：ANS和FSE将终结哈夫曼编码和算术编码在压缩领域内几十年的“霸主”地位。
## C6 自适应统计编码
- C5中统计编码的局限性
    - 第5章介绍的所有统计编码算法，在编码开始之前都需要遍历一次数据，以计算出各符号出现的概率。这样做是有缺点的，为了计算概率总需要多遍历一次数据集，而在计算出整个数据集中各符号的出现概率后，还要继续处理这些数值。如果是相对较小的数据集，那么这些就不是什么问题
    - 然而，随着要压缩的数据集变大，统计编码的结果与熵的偏差也会越来越大，这是因为数据集的不同部分有着不同的概率特征。如果处理的是流数据，比如视频流或音频流，由于整个数据集没有“结尾”，因此就不能“遍历两次”。
- 局部偏态（locality-dependentskewing）
    - 局部性很重要（locality matters）
- 我们编码时不再提前扫描并去找合适的分割点，而是允许统计编码算法自动“重置”
- 这种具有适应数据流熵的局部特性能力的统计编码算法，通常被称为“动态”或“自适应”统计编码算法。这些算法变体构成了大多数重要的、高性能的、高压缩率的多媒体数据流（如图片、视频及音频）压缩算法的基础
### 自适应VLC编码
- 一般来说，统计压缩有3个步骤：(1) 遍历数据流并计算各个符号的出现概率；(2) 根据概率为符号生成VLC；(3) 再次遍历数据流并输出对应的码字。
    - 压缩时需要遍历（或者说扫描）数据流两次，并且整个数据集只有一套VLC表。这里的问题是，VLC表是静态的
- 动态创建VLC表的原理如下。在编码器处理数据流时，每读取一个符号，编码器都会问：•这个符号之前出现过吗？•如果出现过，那么输出当前分配的码字，并更新其出现的概率。•如果没有，则进行一些特殊处理（字面值词条（literal tokens））。
- 自适应统计编码的真正强大之处在于，当输出流的熵要变大失控时，它能重置输出流, 用RESET token
- 自适应算术编码与自适应哈夫曼编码
### 现代的选择
- 相比静态的方法，这些动态的改进有以下优点
    - 有生成符号码字对应表的能力，无须将符号码字对应表显式地存储在数据流中。数据流变小后，计算性能就能有所提高，但更重要的是下面两个优点
    - 有实时压缩数据的能力，无须再将整个数据集作为一个整体来处理。这让我们可以有效地处理更大的数据集，甚至都不用事先知道要处理的数据集有多大
    - 有适应信息局部性的能力，即邻近的符号会对码字的长度有影响，这可以显著提高压缩率。
## C7 字典转换
- 统计压缩主要关注数据流中单个符号的出现概率，这一概率与其周围可能出现的符号无关
- 字典转换实际是一个数据流的预处理阶段，经过这样的预处理后，生成的数据集会更小，比源数据流压缩率更高
- symbol -> word